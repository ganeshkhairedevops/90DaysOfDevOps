# Day 20 â€“ Bash Scripting Challenge: Log Analyzer & Report Generator

Today I built a real-world log analysis automation script.

The script processes system log files, identifies errors and critical events,
and generates a structured summary report.

---

## ğŸ”¹ Approach

### 1ï¸âƒ£ Input Validation
- Checked if log file argument was provided
- Verified file existence before processing
- Used strict mode (set -euo pipefail)

---

### 2ï¸âƒ£ Error Counting
Used:
grep -E "ERROR|Failed"
wc -l

To count total error occurrences.

---

### 3ï¸âƒ£ Critical Events
Used:
grep -n "CRITICAL"

To print line numbers along with event messages.

---

### 4ï¸âƒ£ Top Error Messages
Pipeline used:
grep â†’ awk â†’ sort â†’ uniq -c â†’ sort -rn â†’ head -5

Tools used:
- grep (pattern matching)
- awk (field manipulation)
- sort
- uniq
- wc

---

### 5ï¸âƒ£ Report Generation
Generated a structured report:
log_report_<date>.txt

Included:
- Date
- Log file name
- Total lines processed
- Total error count
- Top 5 errors
- Critical events

---

### 6ï¸âƒ£ Archive Feature
- Created archive/ directory if not present
- Moved processed log file into archive/

---

## ğŸ”¹ Sample Output

Total Errors Found: 87
Report generated: log_report_2026-02-16.txt
Log file moved to archive/

---

## ğŸ§  What I Learned

- Log analysis is about pattern extraction and aggregation
- Bash pipelines are powerful for text processing
- Structured reports improve operational visibility
- Always validate inputs in automation scripts

---

## ğŸ”¹ Commands Used

grep
awk
sort
uniq
wc
date
mv
mkdir
